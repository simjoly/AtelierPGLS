
# Le modèle de régression linéaire {#linear-regression}

## Théorie

Le modèle linéaire a la forme suivante :

$$\textbf{y} = \alpha + \beta \textbf{x} + \textbf{e}$$

$\textbf{y}$ est la variable de réponse (ou dépendante), $\textbf{x}$ est la variable explicative (ou indépendante), et $\textbf{e}$ représente les résidus ou en d'autres termes la variation non expliqué par le modèle. Pour un modèle de régression linéaire simple, cela représente la distance entre les observations (c'est-à-dire les données réelles) et la ligne de régression (c'est-à-dire la prédiction du modèle) le long de l'axe $y$. Le paramètre $\alpha$ représente l'ordonnée à l'origine, qui est la valeur $y$ où la ligne de régression croise l'axe $y$, tandis que le paramètre $\beta$ représente la pente de la ligne. En pratique, vous prenez un échantillon de taille $N$ et vous obtenez des estimations $\hat{\alpha}$ et $\hat{\beta}$ pour l'ordonnée à l'origine et la pente, respectivement. Lorsque la régression linéaire est ajustée à l'aide des moindres carrés ordinaires (OLS), les résidus $\textbf{e}$ sont supposés être normalement distribués avec une espérance $0$ et une variance $\sigma^2$. En termes mathématiques, $\textbf{e} \sim N(0,\sigma^2)$.

L'obtention d'estimations fiables avec une régression linéaire implique que les données répondent à plusieurs hypothèses, parmi lesquelles la normalité, l'homogénéité, le $X$ fixe, l'indépendance et la spécification correcte du modèle. Nous n'examinerons pas tous ces éléments ici, mais nous nous concentrerons sur celui qui est souvent violé lorsque les données sont structurées phylogénétiquement, à savoir l'**indépendance**. Cette hypothèse est importante car un manque d’indépendance invalide des tests importants tels que le test F et le test t.

Vous obtenez une violation de l'indépendance lorsque la valeur $\textbf{y}_i$ à $\textbf{x}_i$ est influencée par d'autres $\textbf{x}_i$. Évidemment, cela peut se produire avec des données phylogénétiquement structurées, car une variable de réponse est plus susceptible de réagir de la même manière chez des espèces étroitement apparentées, car elles partagent de nombreux caractères par voie décente. En d'autres termes, la valeur $y$ d'une espèce n'est pas complètement indépendante de la valeur $y$ d'une espèce étroitement apparentée : leurs $y$ sont corrélés. Nous allons illustrer cela dans un exemple ci-dessous.


## Pratique

Pour fournir des exemples pratiques dans cet atelier, nous utiliserons un ensemble de données de traits fonctionnels d'arbres de la province de Québec [@paquette2015explaining]. L'ensemble de données comprend un certain nombre de traits fonctionnels des plantes et une phylogénie moléculaire construite à l'aide des marqueurs de codes-barres végétaux *rbc*L et *mat*K. L'ensemble de données dont vous avez besoin pour exécuter les exemples se trouve déjà dans le dossier /data/ du référentiel github. Cependant, vous pouvez également les télécharger en cliquant sur les liens ci-dessous.

[seedplants.tre](https://simjoly.github.io/ComparativeMethods-HalfDayWorkshop/data/seedplants.tre)

[seedplants.csv](https://simjoly.github.io/ComparativeMethods-HalfDayWorkshop/data/seedplants.csv)

Avant d'analyser les données, nous commencerons par ouvrir les données et l'arbre phylogénétique et les nettoyer pour ne conserver que les espèces présentes à la fois dans l'arbre et dans le tableau des traits. Cela est nécessaire car certaines espèces supplémentaires ont été incluses dans l'analyse phylogénétique.

```{r "Open_seed_plant_data", warning=FALSE, fig.align='center', fig.height=5}
require(ape)
# Ouvrir les documents ; cela suppose que vous êtes dans le répertoire principal du dossier de l'atelier
seedplantstree <- read.nexus("./data/seedplants.tre")
seedplantsdata <- read.csv2("./data/seedplants.csv")
# Supprimer les espèces pour lesquelles nous n'avons pas de données complètes
seedplantsdata <- na.omit(seedplantsdata)
# Supprimer les espèces de l'arbre qui ne sont pas dans la matrice de données
species.to.exclude <- seedplantstree$tip.label[!(seedplantstree$tip.label %in% seedplantsdata$Code)]
seedplantstree <- drop.tip(seedplantstree,species.to.exclude)
# Supprimer l'objet inutile
rm(species.to.exclude)
# Ordonner l'arbre pour qu'il soit plus esthétique lors du tracé
seedplantstree <- ladderize(seedplantstree, right = FALSE)
# Maintenant, regardons l'arbre
plot(seedplantstree,cex=0.4)
```

Maintenant, nous pouvons jeter un œil aux données, puis ordonner le trait des plantes pour qu'il soit dans le même ordre que les espèces de l'arbre.

```{r "data ordering"}
# Voici à quoi ressemblent les données chargées
head(seedplantsdata)
# Nommer les lignes du data.frame avec les codes des espèces utilisés comme étiquettes d'arbre 
rownames(seedplantsdata) <- seedplantsdata$Code
# Ordonner les données dans le même ordre que les étiquettes de l'arbre. Dans cet exemple 
#  précis, c'était déjà le cas, mais c'est une étape importante pour toute analyse.
seedplantsdata <- seedplantsdata[seedplantstree$tip.label,]
```

Maintenant que les données sont prêtes, réalisons un modèle linéaire et essayons d'expliquer la tolérance à l'ombre (Ombre) des arbres en utilisant la densité du bois (Wd). En R, un moyen très simple de faire une régression consiste à utiliser la fonction « lm », qui signifie modèle linéaire. Pour ajuster un modèle linéaire, vous devez indiquer à la fonction `lm` quelle variable est la variable de réponse et laquelle est la variable explicative. Cela se fait à l'aide de formules sous la forme « Shade ~ Wd ». La variable à gauche du tilde (« ~ ») est la variable de réponse (« Shade ») tandis que les variables explicatives (1 ou plus) sont à droite du tilde.

```{r "Example: non independence", warning=FALSE}
# Ajuster un modèle linéaire en utilisant les moindres carrés ordinaires (MCO)
shade.lm <- lm(Shade ~ Wd, data = seedplantsdata)
# Imprimer les résultats
summary(shade.lm)
```

Vous pouvez voir que l'estimation de la pente (ici le paramètre `Wd`) est `r round(shade.lm$coefficient[2],2)` et ce n'est pas significatif ($p$=`r round(summary(shade.lm)$coefficients[2,4],3)`). Les tracés descriptifs standard obtenus avec `plot(shade.lm)` montrent qu'il existe une variation légèrement plus grande dans les résidus pour les valeurs ajustées faibles, mais celles-ci ne sont pas extrêmes. Cependant, l'hypothèse d'indépendance peut également être violée si les résidus sont phylogénétiquement corrélés. Une façon de tester cela consiste à tracer les résidus aux extrémités de la phylogénie. Voyons ce que cela donne.

```{r "Residuals of lm on phylogeny", fig.align='center'}
# Extraire les résidus
shade.res <- residuals(shade.lm)

#
# Tracer les résidus à côté de la phylogénie

# La commande suivante modifie les paramètres graphiques pour un meilleur rendu de l'arbre
op <- par(mar=c(1,1,1,1))
# Vecteur de couleurs pour le tracé de l'arbre
cols <- c("#7570b3","#d95f02")
# Les trois commandes suivantes vont tracer l'arbre, puis des cercles reflétant 
#  les valeurs résiduelles aux extrémités de l'arbre, et enfin
#  ajouter une légende.
# La commande plot trace l'arbre et laisse un espace pour tracer les 
#  résidus aux extrémités avec l'option 'label.offset=0.01'
plot(seedplantstree,type="p",TRUE,label.offset=0.01,cex=0.5,no.margin=FALSE)
# La commande suivante trace les résidus. l'option 'bg' est pour la couleur de fond.
#  Si les résidus sont supérieurs à 0 (shade.res>0), il affichera la première couleur
#  (1) du tableau 'cols' et s'il est inférieur à zéro, il affiche la deuxième couleur (2). 
#  La taille du cercle (l'option 'cex') est relative à la valeur absolue 
#  des résidus (abs(shade.res). Pour tracer d'autres valeurs, remplacez simplement le 
#  vecteur 'shade.res' par un autre.
tiplabels(pch=21,bg=cols[ifelse(shade.res>0,1,2)],col="black",cex=abs(shade.res),adj=0.505)
# Imprimer la légende
legend("topleft",legend=c("-2","-1","0","1","2"),pch=21,
       pt.bg=cols[c(1,1,1,2,2)],bty="n",
       text.col="gray32",cex=0.8,pt.cex=c(2,1,0.1,1,2))
# Réinitialiser les paramètres graphiques par défaut
par(op) 
```

Vous pouvez voir que dans plusieurs cas, des espèces étroitement apparentées ont tendance à avoir des résidus similaires (ils sont de la même couleur, ce qui signifie qu'ils sont du même côté de la pente de régression). C'est problématique. En effet, cela montre que l’hypothèse d’indépendance de la régression des moindres carrés ordinaires (MCO) n’est plus valable et que les tests statistiques pour les hypothèses nulles ne sont plus valides. Nous verrons ensuite comment les moindres carrés généralisés phylogénétiques peuvent corriger cela.

## Défi no. 1

Dans le data frame `seedplantsdata`, il y avait plusieurs traits différents. Essayez d'ajuster une régression de la tolérance à l'ombre des arbres (`Shade`) en fonction de la masse des graines (`Sm`). En d'autres termes, testez si la tolérance à l'ombre peut être expliquée par la masse des graines des arbres. Ensuite, essayez de voir si les résidus sont corrélés phylogénétiquement.

```{r "Challenge 1", warning=FALSE, echo=FALSE, eval=FALSE}
# Ajuster un modèle linéaire en utilisant les moindres carrés ordinaires (MCO)
Sm.lm <- lm(Shade ~ Sm, data = seedplantsdata)
# Obtenir les résultats
summary(Sm.lm)
# Extraire les résidus
Sm.res <- residuals(Sm.lm)
# Tracer les résidus à côté de la phylogénie
op <- par(mar=c(1,1,1,1))
plot(seedplantstree,type="p",TRUE,label.offset=0.01,cex=0.5,no.margin=FALSE)
tiplabels(pch=21,bg=cols[ifelse(Sm.res>0,1,2)],col="black",cex=abs(Sm.res),adj=0.505)
legend("topleft",legend=c("-2","-1","0","1","2"),pch=21,
       pt.bg=cols[c(1,1,1,2,2)],bty="n",
       text.col="gray32",cex=0.8,pt.cex=c(2,1,0.1,1,2))
par(op)
```
